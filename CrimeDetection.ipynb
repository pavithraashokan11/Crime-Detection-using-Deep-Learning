{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db06fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout,MaxPooling2D , Conv2D,Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1d232c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6724532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = r'C:\\Users\\pavit\\OneDrive\\Desktop\\data\\Train.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf20871",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with zipfile.ZipFile(file, 'r') as zip_ref:\n",
    "#    zip_ref.extractall('data/raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd42baa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = r'C:\\Users\\pavit\\OneDrive\\Desktop\\data\\Test.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b83cba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with zipfile.ZipFile(file, 'r') as zip_ref:\n",
    "#    zip_ref.extractall('data/raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2722707b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce46a083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368fb87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import zipfile\n",
    "\n",
    "# # Replace 'test_shared_link_here' with the actual shareable link for \"Test.zip\"\n",
    "# test_url = 'https://drive.google.com/drive/u/0/recent'\n",
    "\n",
    "# test_response = requests.get(test_url)\n",
    "\n",
    "# # Replace 'Test.zip' with the desired file name\n",
    "# with open('Test.zip', 'wb') as file:\n",
    "#     file.write(test_response.content)\n",
    "\n",
    "# # Extract \"Test.zip\" to a specific folder\n",
    "# with zipfile.ZipFile('Test.zip', 'r') as test_zip_ref:\n",
    "#     test_zip_ref.extractall('path_to_test_extraction_folder')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9942a71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf232a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = r\"C:\\Users\\pavit\\Downloads\\data\\Train\"\n",
    "test_dir = r\"C:\\Users\\pavit\\Downloads\\data\\Test\"\n",
    "\n",
    "SEED = 12\n",
    "IMG_HEIGHT = 64\n",
    "IMG_WIDTH = 64\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 1\n",
    "LR =  0.00003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200d81ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_types=os.listdir(train_dir)\n",
    "n=len(crime_types)\n",
    "print(\"Number of crime categories : \",n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d177ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes={}\n",
    "train=test=0\n",
    "for clss in crime_types:\n",
    "    num=len(os.listdir(os.path.join(train_dir,clss)))\n",
    "    train+=num\n",
    "    test+=len(os.listdir(os.path.join(test_dir,clss)))\n",
    "    \n",
    "    crimes[clss]=num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03a340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.pie(x=np.array([train,test]), autopct=\"%.1f%%\", explode=[0.1, 0.1], labels=[\"Training Data\", \"Test Data\"], pctdistance=0.5)\n",
    "plt.title(\"Share of train and test images \", fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cf38ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 6))\n",
    "plt.barh(list(crimes.keys()), list(crimes.values()), height=0.6, align=\"center\")\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.xlabel(\"Frame count\", fontsize=14)\n",
    "plt.ylabel(\"Classification\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32487e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,10))\n",
    "plt.pie(x=np.array(list(crimes.values())), autopct=\"%.1f%%\", explode=[0.1]*n,labels=list(crimes.keys()), pctdistance=0.5)\n",
    "plt.title(\"Share of train and test images \", fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace0fcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH=64\n",
    "IMG_HEIGHT=64\n",
    "\n",
    "IMG_SHAPE=(IMG_HEIGHT,IMG_WIDTH)\n",
    "seed=69\n",
    "\n",
    "INPUT_SHAPE=(IMG_HEIGHT,IMG_WIDTH,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09661941",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    label_mode=\"categorical\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMG_SHAPE,\n",
    "    shuffle=True,\n",
    "    seed=seed,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    ")\n",
    "\n",
    "val_set=image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    label_mode=\"categorical\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMG_SHAPE,\n",
    "    shuffle=True,\n",
    "    seed=seed,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    ")\n",
    "\n",
    "test_set=image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=None,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMG_SHAPE,\n",
    "    shuffle=False,\n",
    "    seed=seed,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c8dcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_learning():\n",
    "    base_model=DenseNet121(include_top=False,input_shape=INPUT_SHAPE,weights=\"imagenet\")\n",
    "    \n",
    "    thr=149\n",
    "    for layers in base_model.layers[:thr]:\n",
    "        layers.trainable=False\n",
    "    \n",
    "    for layers in base_model.layers[thr:]:\n",
    "        layers.trainable=True\n",
    "    \n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4043f9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model=Sequential()\n",
    "    \n",
    "    base_model=transfer_learning()\n",
    "    model.add(base_model)\n",
    "    \n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    \n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(n,activation=\"softmax\",name=\"classification\"))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b41ad9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model=create_model()\n",
    "\n",
    "model.compile(optimizer=\"adam\", \n",
    "                loss='categorical_crossentropy',\n",
    "                metrics = [tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0131018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(x = train_set,validation_data=val_set,epochs = EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c973cb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a smaller subset of your data for testing\n",
    "# Assuming you have a large dataset with X_train and y_train\n",
    "# Create a smaller subset with 10% of the data\n",
    "#subset_size = int(0.1 * len(train_set))\n",
    "#train_set = train_set.take(subset_size)\n",
    "\n",
    "# Train your model on the smaller subset\n",
    "#history = model.fit(train_set, validation_data=val_set, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c742b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a smaller subset of your data for testing\n",
    "# Assuming you have a large dataset with X_train and y_train\n",
    "# Create a smaller subset with 5% of the data\n",
    "subset_size = int(0.05 * len(train_set))\n",
    "train_set_subset = train_set.take(subset_size)\n",
    "\n",
    "# Train your model on the smaller subset\n",
    "history_subset = model.fit(train_set_subset, validation_data=val_set, epochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfd7534",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true =  np.array([])\n",
    "\n",
    "for x, y in test_set:\n",
    "  y_true = np.concatenate([y_true, np.argmax(y.numpy(), axis=-1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9e348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af72b5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3935b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5be4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    y_test = lb.transform(y_test)\n",
    "    for (idx, c_label) in enumerate(crime_types):\n",
    "        fpr, tpr, thresholds = roc_curve(y_test[:,idx].astype(int), y_pred[:,idx])\n",
    "        c_ax.plot(fpr, tpr,lw=2, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))\n",
    "    c_ax.plot(fpr, fpr, 'black',linestyle='dashed', lw=4, label = 'Random Guessing')\n",
    "    return roc_auc_score(y_test, y_pred, average=average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7af6f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, c_ax = plt.subplots(1,1, figsize = (5,8))\n",
    "\n",
    "\n",
    "print('ROC AUC score:', multiclass_roc_auc_score(y_true , y_pred  , average = \"micro\"))\n",
    "plt.xlabel('FALSE POSITIVE RATE', fontsize=18)\n",
    "plt.ylabel('TRUE POSITIVE RATE', fontsize=16)\n",
    "plt.legend(fontsize = 11.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8556c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ad495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_cnn_model(base_model):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add the transfer learning base model\n",
    "    model.add(base_model)\n",
    "    \n",
    "    # Add additional convolutional layers\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Global average pooling and dense layers\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(n, activation=\"softmax\", name=\"classification\"))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3575eaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = transfer_learning()\n",
    "\n",
    "# # Create a custom CNN model on top of the transfer learning base model\n",
    "# custom_model = create_custom_cnn_model(base_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3981e517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_cnn_model(base_model):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add the base model\n",
    "    model.add(base_model)\n",
    "    \n",
    "    # Global Average Pooling Layer\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    \n",
    "    # Dense layers\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(n, activation='softmax', name='classification'))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define and compile the base model (transfer learning)\n",
    "base_model = transfer_learning()\n",
    "\n",
    "# Create a custom CNN model on top of the transfer learning base model\n",
    "custom_model = create_custom_cnn_model(base_model)\n",
    "\n",
    "# Compile the custom model\n",
    "custom_model.compile(optimizer=\"adam\", \n",
    "                     loss='categorical_crossentropy',\n",
    "                     metrics=[tf.keras.metrics.AUC()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43efad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 32\n",
    "IMG_WIDTH = 32\n",
    "IMG_SHAPE = (IMG_HEIGHT, IMG_WIDTH, 3)\n",
    "INPUT_SHAPE = (IMG_HEIGHT, IMG_WIDTH, 3)\n",
    "\n",
    "# Compile the custom model\n",
    "custom_model.compile(optimizer=\"adam\", \n",
    "                     loss='categorical_crossentropy',\n",
    "                     metrics=[tf.keras.metrics.AUC()])\n",
    "\n",
    "# Fine-tune the entire model on your dataset\n",
    "fine_tune_epochs = 1  # Adjust the number of fine-tuning epochs as needed\n",
    "custom_model.fit(train_set, validation_data=val_set, epochs=fine_tune_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd8ca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install Pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c15e5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Define the directory containing your image files\n",
    "dataset_dir = r\"C:\\Users\\pavit\\Downloads\\data\\Train\\Arrest\"\n",
    "\n",
    "# Get a list of image file names in the directory\n",
    "image_files = [f for f in os.listdir(dataset_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "\n",
    "num_images_to_display = 5  # You can change this number to display a different number of images\n",
    "\n",
    "for i in range(num_images_to_display):\n",
    "    if i >= len(image_files):\n",
    "        break\n",
    "    image_file = os.path.join(dataset_dir, image_files[i])\n",
    "    image = Image.open(image_file)\n",
    "    image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec08247",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Dataset Directory:\", dataset_dir)\n",
    "print(\"Image Files:\", image_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a57668d",
   "metadata": {},
   "source": [
    "# Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d499e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom CNN model on top of the transfer learning base model\n",
    "def create_custom_cnn_model(base_model):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add the base model\n",
    "    model.add(base_model)\n",
    "\n",
    "    # Global Average Pooling Layer\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "\n",
    "    # Dense layers\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(n, activation='softmax', name='classification'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define and compile the base model (transfer learning)\n",
    "base_model = transfer_learning()\n",
    "\n",
    "# Update the IMG_HEIGHT and IMG_WIDTH to 64\n",
    "IMG_HEIGHT = 64\n",
    "IMG_WIDTH = 64\n",
    "IMG_SHAPE = (IMG_HEIGHT, IMG_WIDTH, 3)\n",
    "INPUT_SHAPE = (IMG_HEIGHT, IMG_WIDTH, 3)\n",
    "\n",
    "# Create a new custom CNN model on top of the transfer learning base model\n",
    "custom_model = create_custom_cnn_model(base_model)\n",
    "\n",
    "# Compile the custom model\n",
    "custom_model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=[tf.keras.metrics.AUC()])\n",
    "\n",
    "# Fine-tune the entire model on your dataset\n",
    "fine_tune_epochs = 3  # Adjust the number of fine-tuning epochs as needed\n",
    "custom_model.fit(train_set, validation_data=val_set, epochs=fine_tune_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53a6b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = custom_model.evaluate(test_set)\n",
    "print(\"Test Loss:\", test_metrics[0])\n",
    "print(\"Test AUC:\", test_metrics[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53ea1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.concatenate([y_true, np.argmax(y.numpy(), axis=-1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b897634",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.concatenate([y_true, np.argmax(y.numpy(), axis=-1)], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199dd6e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa0df5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f242ab49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
